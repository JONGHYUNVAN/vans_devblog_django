"""
MongoDBì—ì„œ Elasticsearchë¡œ ê²Œì‹œë¬¼ ë°ì´í„° ë™ê¸°í™”

VansDevBlogì˜ MongoDB Post ë°ì´í„°ë¥¼ Elasticsearchë¡œ ë™ê¸°í™”í•˜ì—¬
ê²€ìƒ‰ ê¸°ëŠ¥ì„ í™œì„±í™”í•©ë‹ˆë‹¤.
"""

import logging
from datetime import timedelta
from typing import Any, Dict, List

from django.core.management.base import BaseCommand, CommandError
from django.utils import timezone

from search.documents import PostDocument
from search.clients.elasticsearch_client import ElasticsearchClient
from search.clients.mongodb_client import MongoDBClient

logger = logging.getLogger("search")


class Command(BaseCommand):
    help = "MongoDBì˜ ê²Œì‹œë¬¼ì„ Elasticsearchë¡œ ë™ê¸°í™”í•©ë‹ˆë‹¤."

    def add_arguments(self, parser):
        parser.add_argument(
            "--batch-size", type=int, default=50, help="ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸° (ê¸°ë³¸ê°’: 50)"
        )
        parser.add_argument(
            "--force-all", action="store_true", help="ë°œí–‰ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ ëª¨ë“  ê²Œì‹œë¬¼ì„ ë™ê¸°í™”í•©ë‹ˆë‹¤."
        )
        parser.add_argument(
            "--incremental", action="store_true", help="ì¦ë¶„ ë™ê¸°í™”: ìµœê·¼ ì—…ë°ì´íŠ¸ëœ ê²Œì‹œë¬¼ë§Œ ë™ê¸°í™”í•©ë‹ˆë‹¤."
        )
        parser.add_argument(
            "--days", type=int, default=7, help="ì¦ë¶„ ë™ê¸°í™” ì‹œ í™•ì¸í•  ì¼ìˆ˜ (ê¸°ë³¸ê°’: 7ì¼)"
        )
        parser.add_argument(
            "--dry-run", action="store_true", help="ì‹¤ì œ ë™ê¸°í™” ì—†ì´ ì²˜ë¦¬í•  ë°ì´í„°ë§Œ í™•ì¸í•©ë‹ˆë‹¤."
        )
        parser.add_argument(
            "--clear-existing",
            action="store_true",
            help="ê¸°ì¡´ Elasticsearch ë°ì´í„°ë¥¼ ëª¨ë‘ ì‚­ì œí•˜ê³  ìƒˆë¡œ ë™ê¸°í™”í•©ë‹ˆë‹¤.",
        )

    def handle(self, *args, **options):
        self.stdout.write("MongoDB â†’ Elasticsearch ë°ì´í„° ë™ê¸°í™” ì‹œìž‘")
        self.stdout.write("=" * 60)

        try:
            # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            mongo_client = MongoDBClient()
            es_client = ElasticsearchClient()

            # ì—°ê²° ìƒíƒœ í™•ì¸
            self._check_connections(mongo_client, es_client)

            # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ì˜µì…˜)
            if options["clear_existing"] and not options["dry_run"]:
                self._clear_existing_data(es_client)

            # ë™ê¸°í™” ì‹¤í–‰
            if options["incremental"]:
                result = self._incremental_sync(mongo_client, es_client, options)
            else:
                result = self._full_sync(mongo_client, es_client, options)

            # ê²°ê³¼ ì¶œë ¥
            self._print_sync_results(result)

            mongo_client.close()
            self.stdout.write(self.style.SUCCESS("\nðŸŽ‰ ë°ì´í„° ë™ê¸°í™” ì™„ë£Œ!"))

        except Exception as e:
            logger.error(f"Data sync failed: {str(e)}")
            raise CommandError(f"ë°ì´í„° ë™ê¸°í™” ì‹¤íŒ¨: {str(e)}")

    def _check_connections(
        self, mongo_client: MongoDBClient, es_client: ElasticsearchClient
    ):
        """MongoDBì™€ Elasticsearch ì—°ê²° ìƒíƒœ í™•ì¸"""
        self.stdout.write("ðŸ”— ì—°ê²° ìƒíƒœ í™•ì¸...")

        if not mongo_client.check_connection():
            raise CommandError("MongoDB ì—°ê²° ì‹¤íŒ¨!")
        self.stdout.write("MongoDB ì—°ê²° ì„±ê³µ")

        if not es_client.check_connection():
            raise CommandError("Elasticsearch ì—°ê²° ì‹¤íŒ¨!")
        self.stdout.write("Elasticsearch ì—°ê²° ì„±ê³µ")

    def _clear_existing_data(self, es_client: ElasticsearchClient):
        """ê¸°ì¡´ Elasticsearch ë°ì´í„° ì‚­ì œ"""
        self.stdout.write("ðŸ—‘ï¸  ê¸°ì¡´ Elasticsearch ë°ì´í„° ì‚­ì œ ì¤‘...")

        try:
            # ëª¨ë“  ë¬¸ì„œ ì‚­ì œ
            from elasticsearch_dsl import Search

            s = Search().using(es_client.client)
            response = s.delete()
            self.stdout.write(f"ì‚­ì œëœ ë¬¸ì„œ ìˆ˜: {response.get('deleted', 0)}ê°œ")
        except Exception as e:
            self.stdout.write(self.style.WARNING(f"ë°ì´í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {str(e)}"))

    def _full_sync(
        self,
        mongo_client: MongoDBClient,
        es_client: ElasticsearchClient,
        options: Dict[str, Any],
    ) -> Dict[str, int]:
        """ì „ì²´ ë™ê¸°í™” ì‹¤í–‰"""
        self.stdout.write("ì „ì²´ ë™ê¸°í™” ì‹œìž‘...")

        batch_size = options["batch_size"]
        force_all = options["force_all"]
        dry_run = options["dry_run"]

        result = {"processed": 0, "synced": 0, "skipped": 0, "errors": 0}

        # ê²Œì‹œë¬¼ ê°€ì ¸ì˜¤ê¸°
        posts_iterator = (
            mongo_client.get_all_posts(batch_size=batch_size)
            if force_all
            else mongo_client.get_all_published_posts(batch_size=batch_size)
        )

        batch_posts = []

        for post in posts_iterator:
            result["processed"] += 1

            # ë°°ì¹˜ ì²˜ë¦¬
            batch_posts.append(post)

            if len(batch_posts) >= batch_size:
                batch_result = self._process_batch(batch_posts, dry_run)
                self._update_result(result, batch_result)
                batch_posts = []

                # ì§„í–‰ ìƒí™© ì¶œë ¥
                self.stdout.write(
                    f"ì²˜ë¦¬ ì¤‘... {result['processed']}ê°œ | "
                    f"ë™ê¸°í™”: {result['synced']}ê°œ | "
                    f"ê±´ë„ˆëœ€: {result['skipped']}ê°œ"
                )

        # ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬
        if batch_posts:
            batch_result = self._process_batch(batch_posts, dry_run)
            self._update_result(result, batch_result)

        return result

    def _incremental_sync(
        self,
        mongo_client: MongoDBClient,
        es_client: ElasticsearchClient,
        options: Dict[str, Any],
    ) -> Dict[str, int]:
        """ì¦ë¶„ ë™ê¸°í™” ì‹¤í–‰"""
        days = options["days"]
        since_date = timezone.now() - timedelta(days=days)

        self.stdout.write(f"ì¦ë¶„ ë™ê¸°í™”: {since_date.strftime('%Y-%m-%d')} ì´í›„ ì—…ë°ì´íŠ¸")

        batch_size = options["batch_size"]
        dry_run = options["dry_run"]

        result = {"processed": 0, "synced": 0, "skipped": 0, "errors": 0}

        batch_posts = []

        for post in mongo_client.get_posts_updated_since(
            since_date, batch_size=batch_size
        ):
            result["processed"] += 1

            batch_posts.append(post)

            if len(batch_posts) >= batch_size:
                batch_result = self._process_batch(batch_posts, dry_run)
                self._update_result(result, batch_result)
                batch_posts = []

                self.stdout.write(
                    f"ì²˜ë¦¬ ì¤‘... {result['processed']}ê°œ | " f"ë™ê¸°í™”: {result['synced']}ê°œ"
                )

        # ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬
        if batch_posts:
            batch_result = self._process_batch(batch_posts, dry_run)
            self._update_result(result, batch_result)

        return result

    def _process_batch(
        self, posts: List[Dict[str, Any]], dry_run: bool
    ) -> Dict[str, int]:
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ê²Œì‹œë¬¼ ì²˜ë¦¬"""
        batch_result = {"synced": 0, "skipped": 0, "errors": 0}

        for post in posts:
            try:
                # ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
                if not self._validate_post_data(post):
                    batch_result["skipped"] += 1
                    continue

                if dry_run:
                    self.stdout.write(
                        f"[DRY-RUN] ë™ê¸°í™” ì˜ˆì •: {post.get('title', 'No Title')[:30]}..."
                    )
                    batch_result["synced"] += 1
                    continue

                # Elasticsearch ë¬¸ì„œ ìƒì„± ë° ì €ìž¥
                es_doc = PostDocument.create_from_mongo_post(post)
                es_doc.save()

                batch_result["synced"] += 1
                logger.debug(f"Synced post: {post.get('_id')}")

            except Exception as e:
                batch_result["errors"] += 1
                logger.error(f"Failed to sync post {post.get('_id')}: {str(e)}")
                self.stdout.write(
                    self.style.WARNING(
                        f"âš ï¸  ë™ê¸°í™” ì‹¤íŒ¨: {post.get('title', 'Unknown')[:30]}..."
                    )
                )

        return batch_result

    def _validate_post_data(self, post: Dict[str, Any]) -> bool:
        """ê²Œì‹œë¬¼ ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬"""
        required_fields = ["_id", "title"]

        for field in required_fields:
            if not post.get(field):
                logger.warning(
                    f"Missing required field '{field}' in post: {post.get('_id')}"
                )
                return False

        return True

    

    def _update_result(
        self, total_result: Dict[str, int], batch_result: Dict[str, int]
    ):
        """ë°°ì¹˜ ê²°ê³¼ë¥¼ ì „ì²´ ê²°ê³¼ì— ë°˜ì˜"""
        for key in batch_result:
            if key in total_result:
                total_result[key] += batch_result[key]

    def _print_sync_results(self, result: Dict[str, int]):
        """ë™ê¸°í™” ê²°ê³¼ ì¶œë ¥"""
        self.stdout.write("\n" + "=" * 60)
        self.stdout.write("ë™ê¸°í™” ê²°ê³¼:")
        self.stdout.write("-" * 30)
        self.stdout.write(f"ì²˜ë¦¬ëœ ê²Œì‹œë¬¼: {result['processed']}ê°œ")
        self.stdout.write(f"ë™ê¸°í™” ì„±ê³µ: {result['synced']}ê°œ")
        self.stdout.write(f"ê±´ë„ˆëœ€: {result['skipped']}ê°œ")
        self.stdout.write(f"ì˜¤ë¥˜: {result['errors']}ê°œ")

        if result["processed"] > 0:
            success_rate = (result["synced"] / result["processed"]) * 100
            self.stdout.write(f"ì„±ê³µë¥ : {success_rate:.1f}%")

        self.stdout.write("=" * 60)
